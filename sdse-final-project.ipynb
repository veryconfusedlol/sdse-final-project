{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be0b0e3",
   "metadata": {},
   "source": [
    "# **SDSE Final Project: Predicting Car Fuel Efficiency with Machine Learning**\n",
    "\n",
    "Our notebook attempts to predict a given car's **combined fuel efficiency (MPG)** based on information that is available **before** buying the car:\n",
    "\n",
    "- Make and type                 (SUV, sedan, etc.)\n",
    "- Drivetrain                    (FWD, AWD, etc.)\n",
    "- Engine displacement           (L)\n",
    "- Number of cylinders           (dimensinoless)\n",
    "- Transmission type             (automatic, manual, etc.)\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "- Consumers can estimate real-world fuel costs for a car model even when independent test results are not widely available yet, such with new vehicle models and trims.\n",
    "- Dealerships and fleet managers can compare many options quickly based on expected efficiency.\n",
    "- Policy or sustainability teams can more immediately simulate how changing the mix of vehicles (more small engines, more hybrids, etc.) might affect fuel consumption and greenhouse/noise emissions.\n",
    "\n",
    "The goal of this notebook is to explain how our code works:\n",
    "\n",
    "1. Load and clean the dataset  \n",
    "2. Encode categorical features and inputs/outputs of interest  \n",
    "3. Train and evaluate a linear regression model  \n",
    "4. Visualize one of the learned relationships  \n",
    "5. Explore the categorical variables  \n",
    "6. Prepare data for and train several neural network architectures\n",
    "\n",
    "Each section has code and an explanation of why it is written this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38d46d",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "Our code starts by importing:\n",
    "\n",
    "- **pandas / numpy** for data manipulation  \n",
    "- **matplotlib** for plotting  \n",
    "- **scikit‑learn** tools for preprocessing and linear regression  \n",
    "- **TensorFlow / Keras** for building neural networks\n",
    "\n",
    "These libraries cover the whole workflow from raw CSV → features → models → evaluation → plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1a388e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, BatchNormalization\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79007a2",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "\n",
    "Next, we read `car_data.csv` into a pandas DataFrame.  \n",
    "Printing the first 5 rows helps verify that:\n",
    "\n",
    "- The file path is correct  \n",
    "- Columns have the expected names\n",
    "- There are no obvious parsing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "data = pd.read_csv(\"car_data.csv\")\n",
    "print(\"First 5 rows of data:\")\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56c718",
   "metadata": {},
   "source": [
    "## 3. Defining Feature Columns and Target\n",
    "\n",
    "Next, we decide which columns will be used as inputs (features) and which column is the output (target).\n",
    "\n",
    "- `categorical` lists all string‑based variables we might want to explore.\n",
    "- `numerical` contains the numeric engine characteristics that are important for fuel use.\n",
    "- `output` is the target: combined MPG.\n",
    "- `categorical_for_model` is a refined list of categorical columns used in the actual model.  \n",
    "  We keep a fixed set of five categorical variables as required by the project guidelines, and avoid overly specific identifiers which would be difficult to generalize well, such as the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60417572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature lists\n",
    "categorical = ['type', 'drive', 'make', 'model', 'transmission']\n",
    "numerical = ['cylinders', 'displacement']\n",
    "output = 'combination_mpg'\n",
    "categorical_for_model = ['type', 'drive', 'fuel_type', 'make', 'transmission']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf091a4",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preparation\n",
    "\n",
    "Before training any model, our code handles missing values:\n",
    "\n",
    "1. **Drop rows with missing target**:  \n",
    "   If `combination_mpg` is missing, the data is unusable for supervised learning.\n",
    "\n",
    "2. **Impute numerical features with the mean**:  \n",
    "   This maintains a baseline that keeps all existing data while avoiding bias towards any existing value.\n",
    "\n",
    "3. **Impute categorical features with the mode** (most frequent category):  \n",
    "   This preserves the most likely class and keeps categories consistent.\n",
    "\n",
    "After cleaning, our code builds:\n",
    "\n",
    "- `feature_cols`: all columns that will be used as inputs\n",
    "- `X`: the feature matrix  \n",
    "- `y`: the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "data = data.dropna(subset=[output])\n",
    "\n",
    "for col in numerical:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "for col in categorical_for_model:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n",
    "\n",
    "# Defining feature matrix X and target y\n",
    "feature_cols = categorical_for_model + numerical\n",
    "X = data[feature_cols]\n",
    "y = data[output]\n",
    "\n",
    "print(\"\\nFeature columns used for the model:\")\n",
    "print(feature_cols)\n",
    "print(f\"\\nNumber of samples after cleaning: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3797ead",
   "metadata": {},
   "source": [
    "## 5. One‑Hot Encoding and Linear Regression\n",
    "\n",
    "Machine learning models like linear regression work with numbers, not strings.  \n",
    "We therefore need to convert categorical features into a numeric format.\n",
    "\n",
    "We use:\n",
    "\n",
    "- **`ColumnTransformer` with `OneHotEncoder`**  \n",
    "  - Each categorical column is expanded into one binary column per category.\n",
    "  - `handle_unknown='ignore'` ensures the model can handle categories that only appear in the test set.\n",
    "\n",
    "- **`Pipeline`** combining preprocessing and the `LinearRegression` estimator:  \n",
    "  - Keeps the workflow \"clean\"  \n",
    "  - Ensures that the same transformations are applied to training and test data in the correct order\n",
    "\n",
    "We then perform a standard train/test split to estimate generalization performance and compute the R² score as our main metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding and linear regression\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_for_model)\n",
    "    ],\n",
    "    remainder='passthrough'  # keep numerical columns as-is\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Fit the linear regression model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate with R^2\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\nLinear Regression R² Score on test set: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae75bf0",
   "metadata": {},
   "source": [
    "## 6. Visualizing the Relationship with Engine Displacement\n",
    "\n",
    "The full linear regression model uses many features, so it is hard to visualize directly.  \n",
    "To build intuition, we look at a single numeric feature, engine `displacement`, and fit a simple one‑feature linear regression:\n",
    "\n",
    "- Scatter plot: actual `displacement` vs. `combination_mpg`\n",
    "- Trend line: best‑fit straight line using simple linear regression\n",
    "- Equation printed on the plot: helps interpret how MPG changes with displacement\n",
    "\n",
    "This **does not** show the full multivariate model but gives a view of one key relationship in the data that is maximimally understandable to our fleshy human eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644edfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing a simple linear relationship with displacement\n",
    "viz_feature = 'displacement'\n",
    "\n",
    "# Extract single feature and target\n",
    "X_viz = data[[viz_feature]].values\n",
    "y_viz = data[output].values\n",
    "\n",
    "# Fit a simple linear regression for visualization\n",
    "simple_lr = LinearRegression()\n",
    "simple_lr.fit(X_viz, y_viz)\n",
    "\n",
    "# Line for plotting\n",
    "x_line = np.linspace(X_viz.min(), X_viz.max(), 200).reshape(-1, 1)\n",
    "y_line = simple_lr.predict(x_line)\n",
    "\n",
    "# Equation components\n",
    "slope = simple_lr.coef_[0]\n",
    "intercept = simple_lr.intercept_\n",
    "eq_text = f\"MPG = {slope:.3f} × {viz_feature} + {intercept:.3f}\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_viz, y_viz, alpha=0.4, label='Actual Data')\n",
    "plt.plot(x_line, y_line, linewidth=2, label='Trend Line')\n",
    "plt.title(f\"Fuel Efficiency vs. {viz_feature.capitalize()}\")\n",
    "plt.xlabel(viz_feature.capitalize())\n",
    "plt.ylabel(\"Combined MPG\")\n",
    "plt.legend()\n",
    "\n",
    "# Put the equation inside a nice box on the plot\n",
    "plt.text(\n",
    "    0.05, 0.95, eq_text,\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"gray\", alpha=0.7)\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d744a8",
   "metadata": {},
   "source": [
    "## 7. Exploring Categorical Feature Distributions\n",
    "\n",
    "Before or after modeling, it is useful to understand what the data looks like:\n",
    "\n",
    "- How many unique values does each categorical column have?\n",
    "- Which categories dominate the dataset? \n",
    "\n",
    "We print basic counts and then plot simple bar charts for each categorical variable.\n",
    "\n",
    "This is important because:\n",
    "\n",
    "- It visualizes highly imbalanced categories (if present), which may bias the model\n",
    "- Some categories may have very few examples and might not be reliable for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d502779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical feature exploration\n",
    "for col in categorical:\n",
    "    print(f\"\\n{col}: {data[col].nunique()} unique classes\")\n",
    "    print(data[col].value_counts().head(10))\n",
    "\n",
    "for col in categorical:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    data[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28813b",
   "metadata": {},
   "source": [
    "## 8. Preparing Encoded Data for Neural Networks\n",
    "\n",
    "Neural networks require purely numeric input and usually benefit from scaled features. We reuse the same preprocessing logic as for linear regression, but we explicitly:\n",
    "\n",
    "1. Fit the `preprocessor` on the training data only to avoid data leakage.\n",
    "2. Transform both train and test splits into encoded matrices.\n",
    "3. Apply `StandardScaler` (without centering, `with_mean=False`) because the encoded data is sparse.  \n",
    "   Scaling helps optimization algorithms like Adam converge faster, allowing our code to run more feasibly on less powerful computers.\n",
    "4. Convert the resulting sparse matrices to dense arrays, which is what Keras expects by default\n",
    "\n",
    "This ensures a fair comparison between the linear model and the neural networks that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62711386",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ====== PREPARE NN DATA  ======\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Fit preprocessor ONLY on training data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mpreprocessor\u001b[49m.fit(X_train)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 2. Transform train and test sets\u001b[39;00m\n\u001b[32m      7\u001b[39m X_train_enc = preprocessor.transform(X_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# ====== PREPARE NN DATA  ======\n",
    "\n",
    "# 1. Fit preprocessor ONLY on training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# 2. Transform train and test sets\n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_test_enc  = preprocessor.transform(X_test)\n",
    "\n",
    "# 3. Scale features (works with sparse matrices)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_train_enc)\n",
    "\n",
    "X_train_enc = scaler.transform(X_train_enc)\n",
    "X_test_enc  = scaler.transform(X_test_enc)\n",
    "\n",
    "# 4. Convert to dense matrices for Keras\n",
    "X_train_enc = X_train_enc.toarray()\n",
    "X_test_enc  = X_test_enc.toarray()\n",
    "\n",
    "print(\"Final input feature dimension:\", X_train_enc.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7d8d4",
   "metadata": {},
   "source": [
    "## 9. Neural Network Architectures\n",
    "\n",
    "To explore non‑linear relationships, we implement four different neural network models:\n",
    "\n",
    "- **Model A — Simple**:  \n",
    "  - 1 hidden layer with 32 neurons (ReLU)  \n",
    "  - Good baseline, fast to train\n",
    "\n",
    "- **Model B — Deeper**:  \n",
    "  - Two hidden layers (64 → 32 units)  \n",
    "  - CAN model, more complex patterns\n",
    "\n",
    "- **Model C — Regularized with Dropout**:  \n",
    "  - Same idea as Model B but adds `Dropout(0.25)`  \n",
    "  - Helps reduce overfitting by randomly dropping neurons during training\n",
    "\n",
    "- **Model D — Wide with Batch Normalization**:  \n",
    "  - Two wide layers (128 → 64 units)  \n",
    "  - `BatchNormalization` after each hidd**en layer  \n",
    "  - Makes training more stable and can speed up convergence\n",
    "\n",
    "All models use:\n",
    "- **Optimizer**: Adam\n",
    "- **Loss**: Mean Squared Error (MSE)\n",
    "- **Metric**: Mean Absolute Error (MAE), easier to interpret as “average error in MPG”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== NN MODELS  ======\n",
    "\n",
    "def build_model_A(input_dim):\n",
    "    \"\"\"Model A - Simple 1-hidden-layer network.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_dim=input_dim),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_B(input_dim):\n",
    "    \"\"\"Model B - Deeper network with two hidden layers.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_C(input_dim):\n",
    "    \"\"\"Model C - Adds dropout for regularization.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_D(input_dim):\n",
    "    \"\"\"Model D - Wide network using batch normalization.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a967a03",
   "metadata": {},
   "source": [
    "## 10. Training, Early Stopping, and Evaluation\n",
    "\n",
    "We train each neural network using:\n",
    "\n",
    "- Early stopping (`EarlyStopping`) with:  \n",
    "  - `patience=10`: if validation loss does not improve for 10 epochs, training stops  \n",
    "  - `restore_best_weights=True`: we keep the model from the best epoch\n",
    "\n",
    "For each model we:\n",
    "\n",
    "1. Train on the encoded training data with a validation split.\n",
    "2. Evaluate on the encoded test set to obtain MSE  and MAE.\n",
    "3. Plot the training and validation loss curves to see whether the model is overfitting or underfitting.\n",
    "\n",
    "This makes the comparison between different architectures transparent and well‑documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "models = {\n",
    "    \"Model A\": build_model_A(X_train_enc.shape[1]),\n",
    "    \"Model B\": build_model_B(X_train_enc.shape[1]),\n",
    "    \"Model C\": build_model_C(X_train_enc.shape[1]),\n",
    "    \"Model D\": build_model_D(X_train_enc.shape[1]),\n",
    "}\n",
    "\n",
    "history = {}\n",
    "results = {}\n",
    "\n",
    "for name, nn_model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    history[name] = nn_model.fit(\n",
    "        X_train_enc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    test_mse, test_mae = nn_model.evaluate(X_test_enc, y_test, verbose=0)\n",
    "    results[name] = (test_mse, test_mae)\n",
    "    print(f\"{name} — Test MAE: {test_mae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53619b",
   "metadata": {},
   "source": [
    "## 11. Visualizing Training Curves\n",
    "\n",
    "Finally, we plot the training loss and validation loss for each model across epochs.\n",
    "\n",
    "Things to look for:\n",
    "\n",
    "- If validation loss starts increasing while training loss keeps decreasing, then the model is overfitted.  \n",
    "- If both losses are high and do not improve, then the model might be underpowered or features might not be informative enough\n",
    "\n",
    "These plots help justify which neural network architectures are the most appropriate for this prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for each model\n",
    "for name in history:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history[name].history['loss'], label='Train Loss')\n",
    "    plt.plot(history[name].history['val_loss'], label='Val Loss')\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
