{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b028d8b1",
   "metadata": {},
   "source": [
    "# **SDSE Final Project: Predicting Car Fuel Efficiency**\n",
    "Team 56\n",
    "\n",
    "\n",
    "Our notebook develops and tests various models to predict a given car's **combined fuel efficiency (MPG)** based on information that is available **before** buying the car:\n",
    "\n",
    "- Make and type                 (SUV, sedan, etc.)\n",
    "- Drivetrain                    (FWD, AWD, etc.)\n",
    "- Engine displacement           (L)\n",
    "- Number of cylinders           (dimensionless)\n",
    "- Transmission type             (automatic, manual, etc.)\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "- Consumers can estimate real-world fuel costs for a car model even when independent test results are not widely available yet, such with new vehicle models and trims.\n",
    "- Dealerships and fleet managers can compare many options quickly based on expected efficiency.\n",
    "- Policy or sustainability teams can more immediately simulate how changing the mix of vehicles (more small engines, more hybrids, etc.) might affect fuel consumption and greenhouse/noise emissions.\n",
    "\n",
    "The notebook follows the outline specified for the project submissions in the lab session. \n",
    "\n",
    "0. Choose performance metric\n",
    "1. Load data\n",
    "2. Split off the test data set\n",
    "3. Choose the families of model and hyperparameter variations to test.\n",
    "4. Execute (train and evaluate) models\n",
    "5. Choose best model and evaluate with test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe9247",
   "metadata": {},
   "source": [
    "# 0. Performance metric\n",
    "\n",
    "e use the coefficient of determination $R^2$ as our primary comparison metric. For regression, $R^2$ measures how much of the variance in the target is explained\n",
    "by the model, and using $R^2$ provides a good baseline by which to compare dissimilar models against.\n",
    "\n",
    "An $R^2$ value close to 1 indicates that the model explains most of the variance\n",
    "in MPG, while values near 0 indicate that the model does not improve much over\n",
    "predicting the mean. We will compare model families based primarily on their\n",
    "validation $R^2$ scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dbae3",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "In this section we:\n",
    "\n",
    "\n",
    "- **1.1**: import necessary libraries/packages\n",
    "    - **pandas / numpy** for data manipulation  \n",
    "    - **matplotlib** for plotting  \n",
    "    - **scikit‑learn** tools for preprocessing and linear regression  \n",
    "    - **TensorFlow / Keras** for building neural networks\n",
    "- read in the data\n",
    "- clean up the data\n",
    "- plot data\n",
    "- reduce number of classes in categories where there are too many (consolidation)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# !pip install tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "import keras_tuner as kt\n",
    "from matplotlib import rc\n",
    "\n",
    "# Enable full LaTeX support in matplotlib\n",
    "rc('text', usetex=True)\n",
    "rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in raw dataset with specs + fuel efficiency\n",
    "data = pd.read_csv(\"car_data.csv\")\n",
    "# print(\"First five rows of data: \\n\", data.head(5))     # Uncomment if you want a quick peak "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec68a62",
   "metadata": {},
   "source": [
    "### 1.2: Defining Feature Columns and Target\n",
    "\n",
    "Next, we decide which columns will be used as inputs (features) and which column is the output (target).\n",
    "\n",
    "- `categorical` lists all string‑based variables we might want to explore.\n",
    "- `numerical` contains the numeric engine characteristics that are important for fuel use.\n",
    "- `output` is the target: combined MPG.\n",
    "- `categorical_for_model` is a refined list of categorical columns used in the actual model.  \n",
    "  We keep a fixed set of five categorical variables as required by the project guidelines, and avoid overly specific identifiers which would be difficult to generalize well, such as the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec86c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns are inputs (X) and which is the target (y)\n",
    "\n",
    "# Raw categorical columns present in the dataset\n",
    "categorical = ['type', 'drive', 'make', 'model', 'transmission']\n",
    "\n",
    "# Raw numerical columns\n",
    "numerical = ['cylinders', 'displacement']\n",
    "\n",
    "# Target we want to predict\n",
    "output = 'combination_mpg'\n",
    "\n",
    "# Categorical columns we’ll actually use in the model\n",
    "# (we drop 'model' here to avoid too many one-hot columns)\n",
    "categorical_for_model = ['type', 'drive', 'fuel_type', 'make', 'transmission']\n",
    "\n",
    "# Final feature matrix and target vector\n",
    "X = data[categorical_for_model + numerical]\n",
    "y = data[output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eca377",
   "metadata": {},
   "source": [
    "### 1.3: Data Cleaning and Preparation\n",
    "\n",
    "Before training any model, our code handles missing values:\n",
    "\n",
    "1. **Drop rows with missing target**: If `combination_mpg` is missing, the data is unusable for supervised learning.\n",
    "\n",
    "2. **Impute numerical features with the mean**: This maintains a baseline that keeps all existing data while avoiding bias towards any existing value.\n",
    "\n",
    "3. **Impute categorical features with the mode**: This preserves the most likely class and keeps categories consistent.\n",
    "\n",
    "After cleaning, our code builds:\n",
    "\n",
    "- `feature_cols`: all columns that will be used as inputs\n",
    "- `X`: the feature matrix  \n",
    "- `y`: the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "data = data.dropna(subset=[output])\n",
    "\n",
    "for col in numerical:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "for col in categorical_for_model:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccf415",
   "metadata": {},
   "source": [
    "### 1.4: Consolidating categories with too many classes\n",
    "\n",
    "We will consolidate the categories \"make\" and \"type\". We will ignore the \"model\" data in the model since each entry is a different model.\n",
    "\n",
    "The \"make\" category will be reduced to 3 classes by region of the company:\n",
    "1. **Asia:** kia, hyundai, genesis, mazda, honda, acura, subaru, mitsubishi, toyota, nissan, infiniti\n",
    "2. **Europe:** bmw, jaguar, mini, audi, land rover, volvo, volkswagen, aston martin, porsche, bentley, mercedes-benz\n",
    "3. **America:** chevrolet, jeep, gmc, ford, cadillac, buick, ram, roush performance, chrysler\n",
    "\n",
    "The \"type\" category will be reduced to 3 classes by size:\n",
    "1. **Small:** small sport utility vehicle, subcompact car, compact car, two seater, minicompact car, small station wagon\n",
    "2. **Medium:** midsize car, standard sport utility vehicle, small pickup truck, midsize station wagon\n",
    "3. **Large:** large car, minivan, standard pickup truck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94039b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining region lists for make region categories\n",
    "asia = [\"kia\", \"hyundai\", \"genesis\", \"mazda\", \"honda\", \"acura\",\n",
    "    \"subaru\", \"mitsubishi\", \"toyota\", \"nissan\", \"infiniti\"]\n",
    "\n",
    "europe = [\"bmw\", \"jaguar\", \"mini\", \"audi\", \"land rover\", \"volvo\",\n",
    "    \"volkswagen\", \"aston martin\", \"porsche\", \"bentley\", \"mercedes-benz\"]\n",
    "\n",
    "america = [\"chevrolet\", \"jeep\", \"gmc\", \"ford\", \"cadillac\", \"buick\",\n",
    "    \"ram\", \"roush performance\", \"chrysler\"]\n",
    "\n",
    "def consolidate_region(make):\n",
    "    if make in asia:\n",
    "        return \"asia\"\n",
    "    if make in europe:\n",
    "        return \"europe\"\n",
    "    if make in america:\n",
    "        return \"america\"\n",
    "    return \"other\"   # just in case\n",
    "\n",
    "# consolidating make data to make_region\n",
    "data[\"make_region\"] = data[\"make\"].apply(consolidate_region)\n",
    "\n",
    "small = [\"small sport utility vehicle\",\"subcompact car\",\"compact car\",\n",
    "            \"two seater\",\"minicompact car\",\"small station wagon\"]\n",
    "medium = [\"midsize car\", \"standard sport utility vehicle\", \n",
    "            \"small pickup truck\", \"midsize station wagon\"]\n",
    "large = [\"large car\", \"minivan\", \"standard pickup truck\"]\n",
    "\n",
    "def consolidate_size(type):\n",
    "    if type in small:\n",
    "        return \"small\"\n",
    "    if type in medium:\n",
    "        return \"medium\"\n",
    "    if type in large:\n",
    "        return \"large\"\n",
    "    return \"other\"\n",
    "        \n",
    "# consolidating type data to size\n",
    "data[\"size\"] = data[\"type\"].apply(consolidate_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31321adc",
   "metadata": {},
   "source": [
    "### 1.5: Defining the final feature set for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Selecting the final feature set for modeling -----\n",
    "\n",
    "# Consolidated list of categorical features we'll use.\n",
    "# Cleaned versions of raw columns so the model\n",
    "# doesn’t explode into too many one-hot encoded columns.\n",
    "categorical_for_model_consolidated = [\n",
    "    'size',\n",
    "    'drive',\n",
    "    'fuel_type',\n",
    "    'make_region',\n",
    "    'transmission'\n",
    "]\n",
    "\n",
    "# Combine categorical + numerical features to form the full input (X)\n",
    "feature_cols = categorical_for_model_consolidated + numerical\n",
    "\n",
    "# X contains only the columns we want the model to learn from\n",
    "X = data[feature_cols]\n",
    "\n",
    "# y is the target we want to predict — combined MPG\n",
    "y = data[output]\n",
    "\n",
    "print(\"Feature columns used for the model:\")\n",
    "print(feature_cols)\n",
    "\n",
    "print(f\"Number of samples after cleaning: {len(X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d27c8a",
   "metadata": {},
   "source": [
    "## 2. Split off the test dataset:\n",
    "We use an 80-20 split for creating training and test sets from the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "# - Train: used to fit / tune models\n",
    "# - Test: held out until the very end to estimate real-world performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,     # 20% of rows are used for final evaluation\n",
    "    random_state=42    # fixed seed for reproducible results\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389100a",
   "metadata": {},
   "source": [
    "### 2.1: One-hot encoding\n",
    "Machine learning models like linear regression work with numbers, not strings.  \n",
    "We therefore need to convert categorical features into a numeric format that a ML model can understand.\n",
    "\n",
    "We use:\n",
    "\n",
    "- **`ColumnTransformer` with `OneHotEncoder`**  \n",
    "  - Each categorical column is expanded into one binary column per category.\n",
    "  - `handle_unknown='ignore'` ensures the model can handle categories that only appear in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the categorical columns and keep numeric ones as they are.\n",
    "# The preprocessor will be reused for all downstream models.\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # 'cat' transformer:\n",
    "        #   - OneHotEncoder turns each category into a binary column\n",
    "        #   - handle_unknown='ignore' lets us safely see unseen categories in test data\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_for_model_consolidated)\n",
    "    ],\n",
    "    remainder='passthrough'  # numerical columns go through unchanged\n",
    ")\n",
    "\n",
    "# Fit preprocessor ONLY on training data to avoid data leakage\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform train and test sets into numeric matrices\n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_test_enc  = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a147e0d",
   "metadata": {},
   "source": [
    "## 3. Choose families of models & hyperparameter variations to test:\n",
    "\n",
    "**1. Linear Regression**  \n",
    "    hyperparameter to optimize: number of features  \n",
    "    method: forward feature reduction  \n",
    "**2. Ridge & Lasso regression**  \n",
    "    hyperparameter to optimize: alpha  \n",
    "**3. Random forest**  \n",
    "    hyperparameter to optimize: number of features  \n",
    "**4. Neural Networks (MLP)**  \n",
    "    hyperparameters to optimize: number of layers, number of nodes, activation functions  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410f406",
   "metadata": {},
   "source": [
    "## 4. Execute (train and evaluate) chosen models:\n",
    "\n",
    "### 4.1: Linear Regression\n",
    "\n",
    "We start with a plain **linear regression** model using all encoded features:\n",
    "- <u>Input</u>: one-hot encoded categorical features + numerical features  \n",
    "- <u>Goal</u>: find coefficients that best fit `combination_mpg` in a least-squares sense  \n",
    "- <u>Metric</u>:  $R^2$ on the test set, which tells us how much of the variance in MPG is explained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4de5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a plain linear regression estimator\n",
    "model_LR = LinearRegression()\n",
    "\n",
    "# 2. Fit the model on the encoded training data\n",
    "model_LR.fit(X_train_enc, y_train)\n",
    "\n",
    "# 3. Predict on the held-out test data\n",
    "y_pred = model_LR.predict(X_test_enc)\n",
    "\n",
    "# 4. Compute R^2 on the test set (how much variance in y we explain)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Linear Regression  $R^2$ Score on test set: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing linear-regression performance\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "# 1. Predicted vs actual MPG\n",
    "ax1.scatter(y_test, y_pred, alpha=0.5)\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\")\n",
    "ax1.set_xlabel(\"Actual MPG (test set)\")\n",
    "ax1.set_ylabel(\"Predicted MPG\")\n",
    "ax1.set_title(\"Linear Regression: Actual vs Predicted MPG\")\n",
    "\n",
    "\n",
    "# 2. Residuals plot (errors vs prediction)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "ax2.scatter(y_pred, residuals, alpha=0.5)\n",
    "ax2.axhline(0, linestyle=\"--\")\n",
    "ax2.set_xlabel(\"Predicted MPG\")\n",
    "ax2.set_ylabel(\"Residual (Actual - Predicted)\")\n",
    "ax2.set_title(\"Linear Regression: Residuals vs Predicted\")\n",
    "\n",
    "plt.tight_layout() # Adjusts subplot params for a tight layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features after preprocessing\n",
    "n_features_full = X_train_enc.shape[1]\n",
    "\n",
    "# Tunable parameters for LR, Ridge, Lasso\n",
    "n_params_linear = n_features_full + 1\n",
    "\n",
    "print(n_params_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd62ea",
   "metadata": {},
   "source": [
    "#### 4.1.1: Linear Regression with forward feature selection\n",
    "\n",
    "We apply forward feature selection on top of linear regression:\n",
    "\n",
    "- Start with no features, then greedily add the feature that most improves cross-validated $R^2$.\n",
    "- Stop when we reach `n_select` features.\n",
    "- Retrain a linear regression model on this reduced feature set and compare performance.\n",
    "\n",
    "To optimize the `n_select` hyperparameter, we run a parameter sweep with GridSearchCV(), and then use the best (in terms of $R^2$) `n_select`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {'sfs__n_features_to_select': (np.linspace(3,14,num=12)).astype(int)}\n",
    "\n",
    "estimator = model_LR\n",
    "model = Pipeline([('sfs',SequentialFeatureSelector(estimator, direction='forward', cv=5)),('lr',estimator)])\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=10, scoring='r2')\n",
    "grid.fit(X_train_enc, y_train)\n",
    "\n",
    "n_select = grid.best_params_['sfs__n_features_to_select']\n",
    "\n",
    "print(\"Best alpha after parameter optimization: n_features_to_select = \",n_select)\n",
    "print(\"Best cross-validation parameter sweep $R^2$:\", grid.best_score_)\n",
    "\n",
    "\n",
    "# running forward FS with optimal number of parameters to remove\n",
    "\n",
    "#forward selection\n",
    "# we could use the cv cross validation parameter as a hyperparameter to optimize/play with\n",
    "sfs_forward = SequentialFeatureSelector(estimator, n_features_to_select=n_select, direction='forward',cv=5)\n",
    "\n",
    "sfs_forward.fit(X_train_enc,y_train)\n",
    "# Extract mask of selected features\n",
    "selected_mask = sfs_forward.get_support()\n",
    "\n",
    "# Reduce train/test matrices\n",
    "X_train_fs = X_train_enc[:, selected_mask]\n",
    "X_test_fs  = X_test_enc[:, selected_mask]\n",
    "\n",
    "# Fit a new model on selected features\n",
    "model_LR_fs = LinearRegression()\n",
    "model_LR_fs.fit(X_train_fs, y_train)\n",
    "\n",
    "# Predict + evaluate\n",
    "y_pred_forward = model_LR_fs.predict(X_test_fs)\n",
    "r2_forward = r2_score(y_test, y_pred_forward)\n",
    "print(f\"Forward Feature Selection Linear Regression $R^2$ Score on test set: {r2_forward:.3f}\")\n",
    "\n",
    "#Get feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "# print(feature_names)\n",
    "# Extract selected and removed names\n",
    "selected_features = feature_names[selected_mask]\n",
    "removed_features  = feature_names[~selected_mask]\n",
    "\n",
    "print(\"Selected features:\")\n",
    "for f in selected_features:\n",
    "    print(f)\n",
    "\n",
    "print(\"Removed features:\")\n",
    "for f in removed_features:\n",
    "    print(f)\n",
    "    \n",
    "# extracting results and plotting performance vs number of features\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# Parameter column is named \"param_sfs__n_features_to_select\"\n",
    "n_vals = cv_results[\"param_sfs__n_features_to_select\"].astype(int)\n",
    "r2_vals = cv_results[\"mean_test_score\"]\n",
    "\n",
    "# best n_select\n",
    "best_n = n_select\n",
    "best_idx = n_vals[n_vals == best_n].index[0]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_vals, r2_vals, marker='o', linestyle='-', label='CV $R^2$')\n",
    "plt.axvline(best_n, color='red', linestyle='--', label=f'Best n_select = {best_n}')\n",
    "\n",
    "plt.xlabel(\"Number of Selected Features (n_select)\")\n",
    "plt.ylabel(\"Cross-Validated $R^2$\")\n",
    "plt.title(\"Sequential Feature Selection: Validation Performance vs Feature Count\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_sfs = X_train_fs.shape[1]\n",
    "n_params_sfs = n_features_sfs + 1\n",
    "print(n_params_sfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a393273",
   "metadata": {},
   "source": [
    "### 4.2 Ridge Regression\n",
    "We also try Ridge Regression, which is linear regression with L2 penalty:\n",
    "\n",
    "- Adds a penalty on large coefficients to reduce overfitting.\n",
    "- Controlled by hyperparameter α.\n",
    "- We use cross-validation over several α values and choose the one that gives the best average R².\n",
    "\n",
    "The plot shows how the Ridge regularization parameter $\\lambda = \\alpha$ (on a log scale) affects the mean cross-validated $R^2$.\n",
    "\n",
    "We can see where the performance peaks and how quickly it degrades\n",
    "for very small or very large regularization, which supports the choice\n",
    "of the final $\\lambda$ used in the results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search for Ridge Regression (L2-regularized linear model)\n",
    "# We search over different alpha values using 5-fold cross-validation\n",
    "\n",
    "param_grid = {'alpha': np.logspace(-3,3,num=10)}\n",
    "param_grid = {'alpha': np.logspace(-3,0,num=10)}\n",
    "\n",
    "model_ridge = Ridge()\n",
    "\n",
    "grid = GridSearchCV(model_ridge, param_grid, cv=5, scoring='r2')\n",
    "grid.fit(X_train_enc, y_train)\n",
    "\n",
    "\n",
    "best_alpha_ridge = grid.best_params_['alpha']\n",
    "\n",
    "print(\"Best alpha after parameter optimization: alpha = \", best_alpha_ridge)\n",
    "print(\"Best cross-validation parameter sweep R²:\", grid.best_score_)\n",
    "\n",
    "# creating model_ridge with the optimized alpha value\n",
    "model_ridge_best = grid.best_estimator_\n",
    "model_ridge_best.fit(X_train_enc, y_train)\n",
    "\n",
    "y_pred = model_ridge_best.predict(X_test_enc)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Best Ridge Test $R^2$:\", r2)\n",
    "\n",
    "\n",
    "# Plot\n",
    "# Convert ridge CV results to DataFrame\n",
    "results_ridge = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# Extract alpha values as float\n",
    "results_ridge[\"alpha\"] = results_ridge[\"param_alpha\"].astype(float)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(\n",
    "    results_ridge[\"alpha\"],\n",
    "    results_ridge[\"mean_test_score\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    results_ridge[\"alpha\"],\n",
    "    results_ridge[\"mean_test_score\"] - results_ridge[\"std_test_score\"],\n",
    "    results_ridge[\"mean_test_score\"] + results_ridge[\"std_test_score\"],\n",
    "    alpha=0.2\n",
    ")\n",
    "plt.xlabel(r\"$\\lambda = \\alpha$ (log scale)\")\n",
    "plt.ylabel(\"Mean CV R$^2$\")\n",
    "plt.title(\"Ridge regression: regularization vs validation R$^2$\")\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.show()\n",
    "\n",
    "results_ridge[[\"alpha\", \"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379fecb",
   "metadata": {},
   "source": [
    "### 4.3: Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89568831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso model: hyperparameter optimization for alpha\n",
    "\n",
    "param_grid = {'alpha': np.logspace(-3,2,num=10)}\n",
    "\n",
    "model_lasso = Lasso()\n",
    "\n",
    "grid = GridSearchCV(model_lasso, param_grid, cv=5, scoring='r2')\n",
    "grid.fit(X_train_enc, y_train)\n",
    "\n",
    "\n",
    "best_alpha_lasso = grid.best_params_['alpha']\n",
    "\n",
    "print(\"Best alpha after parameter optimization: alpha = \", best_alpha_lasso)\n",
    "print(\"Best cross-validation parameter sweep $R^2$:\", grid.best_score_)\n",
    "\n",
    "# creating model_lasso with the optimized alpha value\n",
    "model_lasso_best = grid.best_estimator_\n",
    "model_lasso_best.fit(X_train_enc, y_train)\n",
    "\n",
    "y_pred = model_lasso_best.predict(X_test_enc)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Best Lasso Test $R^2$:\", r2)\n",
    "\n",
    "results_lasso = pd.DataFrame(grid.cv_results_)\n",
    "results_lasso[\"alpha\"] = results_lasso[\"param_alpha\"].astype(float)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(\n",
    "    results_lasso[\"alpha\"],\n",
    "    results_lasso[\"mean_test_score\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    results_lasso[\"alpha\"],\n",
    "    results_lasso[\"mean_test_score\"] - results_lasso[\"std_test_score\"],\n",
    "    results_lasso[\"mean_test_score\"] + results_lasso[\"std_test_score\"],\n",
    "    alpha=0.2\n",
    ")\n",
    "plt.xlabel(r\"$\\lambda = \\alpha$ (log scale)\")\n",
    "plt.ylabel(\"Mean CV R$^2$\")\n",
    "plt.title(\"Lasso regression: regularization vs validation R$^2$\")\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.show()\n",
    "\n",
    "results_lasso[[\"alpha\", \"mean_test_score\", \"std_test_score\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4db0c4",
   "metadata": {},
   "source": [
    "### 4.4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "model_rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    model_rf,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train_enc, y_train)\n",
    "\n",
    "print(\"Best RF parameters:\", grid.best_params_)\n",
    "print(\"Best CV $R^2$ :\", grid.best_score_)\n",
    "\n",
    "# -----------------------------\n",
    "# Best Random Forest Model\n",
    "# -----------------------------\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "results_rf = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "results_rf[\"n_estimators\"] = results_rf[\"param_n_estimators\"].astype(int)\n",
    "results_rf[\"min_samples_leaf\"] = results_rf[\"param_min_samples_leaf\"].astype(int)\n",
    "\n",
    "#  Add results \n",
    "rf_ne = results_rf.groupby(\"n_estimators\")[\"mean_test_score\"].mean().reset_index()\n",
    "\n",
    "rf_md = results_rf.groupby(\"param_max_depth\")[\"mean_test_score\"].mean().reset_index()\n",
    "rf_md = rf_md.rename(columns={\"param_max_depth\": \"max_depth\"})\n",
    "\n",
    "rf_ms = results_rf.groupby(\"min_samples_leaf\")[\"mean_test_score\"].mean().reset_index()\n",
    "\n",
    "#plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# n_estimators vs CV R^2\n",
    "axes[0].plot(rf_ne[\"n_estimators\"], rf_ne[\"mean_test_score\"], marker=\"o\")\n",
    "axes[0].set_xlabel(\"n_estimators\")\n",
    "axes[0].set_ylabel(\"Mean CV R$^2$\")\n",
    "axes[0].set_title(\"Random Forest: n_estimators vs R$^2$\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# max_depth vs CV R^2\n",
    "axes[1].plot(rf_md[\"max_depth\"], rf_md[\"mean_test_score\"], marker=\"o\")\n",
    "axes[1].set_xlabel(\"max_depth (None = unlimited)\")\n",
    "axes[1].set_ylabel(\"Mean CV R$^2$\")\n",
    "axes[1].set_title(\"Random Forest: max_depth vs R$^2$\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# min_samples_leaf vs CV R^2\n",
    "axes[2].plot(rf_ms[\"min_samples_leaf\"], rf_ms[\"mean_test_score\"], marker=\"o\")\n",
    "axes[2].set_xlabel(\"min_samples_leaf\")\n",
    "axes[2].set_ylabel(\"Mean CV R$^2$\")\n",
    "axes[2].set_title(\"Random Forest: min_samples_leaf vs R$^2$\")\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f93ee",
   "metadata": {},
   "source": [
    "### 4.5: Neural Networks\n",
    "\n",
    "To explore non‑linear relationships, we implement four different neural network models:\n",
    "\n",
    "- **Model A: Simple**:  \n",
    "  - 1 hidden layer with 32 neurons (ReLU)  \n",
    "  - Good baseline, fast to train\n",
    "\n",
    "- **Model B: Deeper**:  \n",
    "  - Two hidden layers (64 → 32 units)  \n",
    "  - CAN model, more complex patterns\n",
    "\n",
    "- **Model C: Regularized with Dropout**:  \n",
    "  - Same idea as Model B but adds `Dropout(0.25)`  \n",
    "  - Helps reduce overfitting by randomly dropping neurons during training\n",
    "\n",
    "- **Model D: Wide with Batch Normalization**:  \n",
    "  - Two wide layers (128 → 64 units)  \n",
    "  - `BatchNormalization` after each hidd**en layer  \n",
    "  - Makes training more stable and can speed up convergence\n",
    "\n",
    "- **Model E: Hyperparameter-Optimized Neural Network (Keras Tuner)**:  \n",
    "  Model E extends our neural network exploration by using automated hyperparameter optimization instead of manually chosen architectures. Using Keras Tuner’s Random Search, the model searches over a configurable design space that includes:\n",
    "\t- Network depth: 1–3 hidden layers\n",
    "\t-\tHidden layer width: 32 to 256 units\n",
    "\t-\tActivation function: ReLU or Tanh\n",
    "\t-\tDropout usage and rate: optional dropout with rates between 0.1–0.4\n",
    "\t-\tLearning rate: log-scaled between 1e-4 and 1e-2\n",
    "\n",
    "  During tuning, the system evaluates multiple candidate architectures and selects the configuration that minimizes validation MAE. A final model is then rebuilt using the best hyperparameters and retrained on the full training set with early stopping. This approach allows Model E to adaptively discover the most effective architecture for the MPG prediction task—potentially achieving better performance than hand-designed networks (Models A–D), especially when nonlinear interactions are complex or when optimal layer sizes and learning rates are not obvious.\n",
    "\n",
    "\n",
    "All models use:\n",
    "- **Optimizer**: Adam\n",
    "- **Loss**: Mean Squared Error (MSE)\n",
    "- **Metric**: Mean Absolute Error (MAE), easier to interpret as “average error in MPG”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a032b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final input feature dimension:\", X_train_enc.shape[1])\n",
    "# nn_rows = results_df[results_df[\"Family\"].str.contains(\"Neural\", case=False)]\n",
    "\n",
    "# if not nn_rows.empty:\n",
    "#     plt.figure()\n",
    "#     plt.bar(nn_rows[\"Model\"], nn_rows[\"MAE\"])\n",
    "#     plt.ylabel(\"Test MAE (MPG)\")\n",
    "#     plt.title(\"Neural network architectures vs test MAE\")\n",
    "#     plt.xticks(rotation=30)\n",
    "#     plt.grid(axis=\"y\")\n",
    "#     plt.show()\n",
    "\n",
    "#     nn_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN models\n",
    "\n",
    "def build_model_A(input_dim):\n",
    "    \"\"\"Model A - Simple 1-hidden-layer network.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_dim=input_dim),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_B(input_dim):\n",
    "    \"\"\"Model B - Deeper network with two hidden layers.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_C(input_dim):\n",
    "    \"\"\"Model C - Adds dropout for regularization.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_D(input_dim):\n",
    "    \"\"\"Model D - Wide network using batch normalization.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_E(hp):\n",
    "    \"\"\"Model FE — Hyperparameter-tuned network using Keras Tuner.\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Number of layers: choose 1–3 hidden layers\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=256, step=32),\n",
    "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "        ))\n",
    "\n",
    "        # Optional dropout after each layer\n",
    "        if hp.Boolean(f\"dropout_{i}\"):\n",
    "            model.add(Dropout(rate=hp.Float(f\"drop_rate_{i}\", 0.1, 0.4, step=0.1)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Learning rate search\n",
    "    lr = hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58776906",
   "metadata": {},
   "source": [
    "We train each neural network using:\n",
    "\n",
    "- Early stopping (`EarlyStopping`) with:  \n",
    "  - `patience=10`: if validation loss does not improve for 10 epochs, training stops  \n",
    "  - `restore_best_weights=True`: we keep the model from the best epoch\n",
    "\n",
    "For each model we:\n",
    "\n",
    "1. Train on the encoded training data with a validation split.\n",
    "2. Evaluate on the encoded test set to obtain MSE  and MAE.\n",
    "3. Plot the training and validation loss curves to see whether the model is overfitting or underfitting.\n",
    "\n",
    "This makes the comparison between different architectures transparent and well‑documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58776906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each neural network variant\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping stops training if val loss doesn't improve for 10 epochs,\n",
    "    # and restores the weights from the best epoch.\n",
    "    EarlyStopping(patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Build one instance of each model architecture\n",
    "models = {\n",
    "    \"Model A\": build_model_A(X_train_enc.shape[1]),\n",
    "    \"Model B\": build_model_B(X_train_enc.shape[1]),\n",
    "    \"Model C\": build_model_C(X_train_enc.shape[1]),\n",
    "    \"Model D\": build_model_D(X_train_enc.shape[1]),\n",
    "}\n",
    "\n",
    "history = {}   # training curves (loss, val_loss)\n",
    "results = {}   # final test metrics\n",
    "\n",
    "for name, nn_model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    history[name] = nn_model.fit(\n",
    "        X_train_enc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    # Evaluate on test data and store MSE/MAE\n",
    "    test_mse, test_mae = nn_model.evaluate(X_test_enc, y_test, verbose=0)\n",
    "    results[name] = (test_mse, test_mae)\n",
    "    print(f\"{name} — Test MAE: {test_mae:.3f}\")\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model_E,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=10,          # Try 10 different architectures\n",
    "    executions_per_trial=1, # How many repeated trainings per trial\n",
    "    directory=\"ktuner_dir\",\n",
    "    project_name=\"model_E_mpg\"\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train_enc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "model_E = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "history[\"Model E\"] = model_E.fit(\n",
    "    X_train_enc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=8, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "test_mse_E, test_mae_E = model_E.evaluate(X_test_enc, y_test, verbose=0)\n",
    "results[\"Model E\"] = (test_mse_E, test_mae_E)\n",
    "\n",
    "print(f\"Model E (Keras Tuner) — Test MAE: {test_mae_E:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643d115",
   "metadata": {},
   "source": [
    "**Visualizing Training Curves**  \n",
    "Finally, we plot the training loss and validation loss for each model across epochs.\n",
    "\n",
    "Things to look for:\n",
    "\n",
    "- If validation loss starts increasing while training loss keeps decreasing, then the model is overfitted.  \n",
    "- If both losses are high and do not improve, then the model might be underpowered or features might not be informative enough\n",
    "\n",
    "These plots help justify which neural network architectures are the most appropriate for this prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = len(history)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_models, figsize=(5*num_models, 4))\n",
    "\n",
    "if num_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, name in zip(axes, history.keys()):\n",
    "    ax.plot(history[name].history['loss'], label='Train Loss')\n",
    "    ax.plot(history[name].history['val_loss'], label='Val Loss')\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3d554",
   "metadata": {},
   "source": [
    "## 5: Choose best model and evaluate with test data:\n",
    "\n",
    "In this final section we compare all candidate models on the held-out test set and pick a\n",
    "single “best” model for predicting combined MPG.\n",
    "\n",
    "We focus on:\n",
    "\n",
    "- **R²** – how much of the variance in MPG the model explains  \n",
    "- **MAE** – average absolute error in MPG (easier to interpret)  \n",
    "- **RMSE** – root mean squared error (penalizes large mistakes)\n",
    "\n",
    "Models we compare:\n",
    "\n",
    "1. **Baseline Linear Regression** (all encoded features)  \n",
    "2. **Ridge Regression** with the best α value (from Section 4.5)  \n",
    "3. **Neural Networks (Models A–D)** from Section 4.X  \n",
    "\n",
    "By evaluating all models on the **same test set**, we can see which one generalizes best,\n",
    "instead of just memorizing the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = []\n",
    "\n",
    "# Plain Linear Regression\n",
    "y_pred_lr_train = model_LR.predict(X_train_enc)\n",
    "y_pred_lr = model_LR.predict(X_test_enc)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "\n",
    "results_summary.append({\n",
    "    \"Model\": \"Linear Regression\",\n",
    "    \"Family\": \"Linear\",\n",
    "    \"R2_train\": r2_score(y_train, y_pred_lr_train),\n",
    "    \"R2\": r2_score(y_test, y_pred_lr),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_lr),\n",
    "    \"RMSE\": rmse_lr,\n",
    "})\n",
    "\n",
    "# Forward Feature Reduced LR\n",
    "y_pred_lr_fs_train = model_LR_fs.predict(X_train_fs)\n",
    "y_pred_lr_fs = model_LR_fs.predict(X_test_fs)\n",
    "mse_lr_fs = mean_squared_error(y_test, y_pred_lr_fs)\n",
    "rmse_lr_fs = np.sqrt(mse_lr_fs)\n",
    "\n",
    "results_summary.append({\n",
    "    \"Model\": \"Feature Reduced Linear Regression\",\n",
    "    \"Family\": \"Linear\",\n",
    "    \"R2_train\": r2_score(y_train, y_pred_lr_fs_train),\n",
    "    \"R2\": r2_score(y_test, y_pred_lr_fs),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_lr_fs),\n",
    "    \"RMSE\": rmse_lr_fs,\n",
    "})\n",
    "\n",
    "# Best Ridge Regression\n",
    "model_ridge_best.fit(X_train_enc, y_train)\n",
    "y_pred_ridge_train = model_ridge_best.predict(X_train_enc)\n",
    "y_pred_ridge = model_ridge_best.predict(X_test_enc)\n",
    "\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "rmse_ridge = np.sqrt(mse_ridge)\n",
    "\n",
    "results_summary.append({\n",
    "    \"Model\": f\"Ridge (alpha={best_alpha_ridge})\",\n",
    "    \"Family\": \"Linear (regularized)\",\n",
    "    \"R2_train\": r2_score(y_train, y_pred_ridge_train),\n",
    "    \"R2\": r2_score(y_test, y_pred_ridge),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_ridge),\n",
    "    \"RMSE\": rmse_ridge,\n",
    "})\n",
    "\n",
    "# Best Lasso Regression\n",
    "model_lasso_best.fit(X_train_enc, y_train)\n",
    "y_pred_lasso_train = model_lasso_best.predict(X_train_enc)\n",
    "y_pred_lasso = model_lasso_best.predict(X_test_enc)\n",
    "\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "rmse_lasso = np.sqrt(mse_lasso)\n",
    "\n",
    "results_summary.append({\n",
    "    \"Model\": f\"Lasso (alpha={best_alpha_lasso})\",\n",
    "    \"Family\": \"Linear (regularized)\",\n",
    "    \"R2_train\": r2_score(y_train, y_pred_lasso_train),\n",
    "    \"R2\": r2_score(y_test, y_pred_lasso),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_lasso),\n",
    "    \"RMSE\": rmse_lasso,\n",
    "})\n",
    "# Train prediction\n",
    "y_pred_rf_train = best_rf.predict(X_train_enc)\n",
    "\n",
    "# Test prediction\n",
    "y_pred_rf = best_rf.predict(X_test_enc)\n",
    "\n",
    "# Errors\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "# Add to results table\n",
    "results_summary.append({\n",
    "    \"Model\": \"Random Forest\",\n",
    "    \"Family\": \"Tree Ensemble\",\n",
    "    \"R2_train\": r2_score(y_train, y_pred_rf_train),\n",
    "    \"R2\": r2_score(y_test, y_pred_rf),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_rf),\n",
    "    \"RMSE\": rmse_rf,\n",
    "})\n",
    "\n",
    "# Neural Networks A–D\n",
    "for name, nn_model in models.items():\n",
    "    y_pred_nn_train = nn_model.predict(X_train_enc).flatten()\n",
    "    y_pred_nn = nn_model.predict(X_test_enc).flatten()\n",
    "    mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "    rmse_nn = np.sqrt(mse_nn)\n",
    "\n",
    "    results_summary.append({\n",
    "        \"Model\": name,\n",
    "        \"Family\": \"Neural Network\",\n",
    "        \"R2_train\": r2_score(y_train, y_pred_nn_train),\n",
    "        \"R2\": r2_score(y_test, y_pred_nn),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred_nn),\n",
    "        \"RMSE\": rmse_nn,\n",
    "    })\n",
    "\n",
    "# Tuned neural network (Model E)\n",
    "y_pred_E_train = model_E.predict(X_train_enc).flatten()\n",
    "y_pred_E = model_E.predict(X_test_enc).flatten()\n",
    "mse_E = mean_squared_error(y_test, y_pred_E)\n",
    "rmse_E = np.sqrt(mse_E)\n",
    "\n",
    "results_summary.append({\n",
    "    \"Model\": \"Model E (Keras Tuner)\",\n",
    "    \"Family\": \"Neural Network (Tuned)\",\n",
    "    \"R2_train\": r2_score(y_train, y_pred_E_train),\n",
    "    \"R2\": r2_score(y_test, y_pred_E),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_E),\n",
    "    \"RMSE\": rmse_E,\n",
    "})\n",
    "\n",
    "# Build results table (now includes training R2)\n",
    "results_df = pd.DataFrame(results_summary).sort_values(by=\"MAE\").reset_index(drop=True)\n",
    "\n",
    "print(\"Test-set performance of all models (with training R² included):\\n\")\n",
    "display(results_df)\n",
    "\n",
    "# Compare training vs test performance using R2\n",
    "results_df[\"R2_gap\"] = results_df[\"R2_train\"] - results_df[\"R2\"]\n",
    "\n",
    "print(\"\\nTraining vs test R² (R²_train - R²_test):\\n\")\n",
    "display(\n",
    "    results_df[[\"Model\", \"Family\", \"R2_train\", \"R2\", \"R2_gap\"]]\n",
    "    .sort_values(by=\"R2_gap\", ascending=False)\n",
    ")\n",
    "\n",
    "best_row = results_df.iloc[0]\n",
    "print(\"Best overall model on the test set:\")\n",
    "print(\n",
    "    f\"  {best_row['Model']} \"\n",
    "    f\"(Family: {best_row['Family']}) \"\n",
    "    f\"— R²={best_row['R2']:.3f}, MAE={best_row['MAE']:.3f}, RMSE={best_row['RMSE']:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a141ac",
   "metadata": {},
   "source": [
    "The scores show how well each model handled the task of predicting a vehicle’s combined MPG:\n",
    "- R² tells us how much of the variation in fuel economy the model can explain — higher is better.\n",
    "- MAE gives the average size of the model’s mistakes in MPG, which is easy to understand in real-world terms.\n",
    "- RMSE punishes bigger errors more heavily and is a good indicator of how stable the model is across different kinds of cars.\n",
    "\n",
    "Comparing these models side-by-side reveals which approach generalized best rather than simply memorizing the training data.\n",
    "The best model delivers the most reliable MPG predictions and is the that would be best for deployment or continued focused refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25b35d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
