{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b028d8b1",
   "metadata": {},
   "source": [
    "# **SDSE Final Project: Predicting Car Fuel Efficiency with Machine Learning**\n",
    "\n",
    "Our notebook attempts to predict a given car's **combined fuel efficiency (MPG)** based on information that is available **before** buying the car:\n",
    "\n",
    "- Make and type                 (SUV, sedan, etc.)\n",
    "- Drivetrain                    (FWD, AWD, etc.)\n",
    "- Engine displacement           (L)\n",
    "- Number of cylinders           (dimensinoless)\n",
    "- Transmission type             (automatic, manual, etc.)\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "- Consumers can estimate real-world fuel costs for a car model even when independent test results are not widely available yet, such with new vehicle models and trims.\n",
    "- Dealerships and fleet managers can compare many options quickly based on expected efficiency.\n",
    "- Policy or sustainability teams can more immediately simulate how changing the mix of vehicles (more small engines, more hybrids, etc.) might affect fuel consumption and greenhouse/noise emissions.\n",
    "\n",
    "The notebook follows the outline specified for the project submissions in the lab session. \n",
    "\n",
    "0. Choose performance metric\n",
    "1. load data\n",
    "2. split off the test data set\n",
    "3. Choose the families of model and hyperparameter variations to test.\n",
    "4. Execute (train and evaluate) models\n",
    "5. Choose best model and evaluate with test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe9247",
   "metadata": {},
   "source": [
    "# 0. Performance metric:\n",
    "\n",
    "Since we are working with a regression model, we will be using the R^2 coefficient of determination as the metric to evaluate all our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dbae3",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "In this section we:\n",
    "\n",
    "\n",
    "- **1.1**: import necessary libraries/packages\n",
    "    - **pandas / numpy** for data manipulation  \n",
    "    - **matplotlib** for plotting  \n",
    "    - **scikit‑learn** tools for preprocessing and linear regression  \n",
    "    - **TensorFlow / Keras** for building neural networks\n",
    "- read in the data\n",
    "- clean up the data\n",
    "- plot data\n",
    "- reduce number of classes in categories where there are too many (consolidation)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9642427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13ea98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "data = pd.read_csv(\"car_data.csv\")\n",
    "# print(First 5 rows of data:\")\n",
    "# print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec68a62",
   "metadata": {},
   "source": [
    "**1.3: Defining Feature Columns and Target**\n",
    "\n",
    "Next, we decide which columns will be used as inputs (features) and which column is the output (target).\n",
    "\n",
    "- `categorical` lists all string‑based variables we might want to explore.\n",
    "- `numerical` contains the numeric engine characteristics that are important for fuel use.\n",
    "- `output` is the target: combined MPG.\n",
    "- `categorical_for_model` is a refined list of categorical columns used in the actual model.  \n",
    "  We keep a fixed set of five categorical variables as required by the project guidelines, and avoid overly specific identifiers which would be difficult to generalize well, such as the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ec86c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature lists\n",
    "categorical = ['type', 'drive', 'make', 'model', 'transmission']\n",
    "numerical = ['cylinders', 'displacement']\n",
    "output = 'combination_mpg'\n",
    "categorical_for_model = ['type', 'drive', 'fuel_type', 'make', 'transmission']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eca377",
   "metadata": {},
   "source": [
    "**1.4: Data Cleaning and Preparation**\n",
    "\n",
    "Before training any model, our code handles missing values:\n",
    "\n",
    "1. **Drop rows with missing target**:  \n",
    "   If `combination_mpg` is missing, the data is unusable for supervised learning.\n",
    "\n",
    "2. **Impute numerical features with the mean**:  \n",
    "   This maintains a baseline that keeps all existing data while avoiding bias towards any existing value.\n",
    "\n",
    "3. **Impute categorical features with the mode** (most frequent category):  \n",
    "   This preserves the most likely class and keeps categories consistent.\n",
    "\n",
    "After cleaning, our code builds:\n",
    "\n",
    "- `feature_cols`: all columns that will be used as inputs\n",
    "- `X`: the feature matrix  \n",
    "- `y`: the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed80294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "data = data.dropna(subset=[output])\n",
    "\n",
    "for col in numerical:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "for col in categorical_for_model:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n",
    "\n",
    "# # Defining feature matrix X and target y\n",
    "# feature_cols = categorical_for_model + numerical\n",
    "# X = data[feature_cols]\n",
    "# y = data[output]\n",
    "\n",
    "# print(\"\\nFeature columns used for the model:\")\n",
    "# print(feature_cols)\n",
    "# print(f\"\\nNumber of samples after cleaning: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccf415",
   "metadata": {},
   "source": [
    "**1.5: Consolidating categories with too many classes**\n",
    "\n",
    "We will consolidate the categories \"make\" and \"type\". We will ignore the \"model\" data in the model since each entry is a different model.\n",
    "\n",
    "The \"make\" category will be reduced to 3 classes by region of the company:\n",
    "1. Asia: kia, hyundai, genesis, mazda, honda, acura, subaru, mitsubishi, toyota, nissan, infiniti\n",
    "2. Europe: bmw, jaguar, mini, audi, land rover, volvo, volkswagen, aston martin, porsche, bentley, mercedes-benz\n",
    "3. America: chevrolet, jeep, gmc, ford, cadillac, buick, ram, roush performance, chrysler\n",
    "\n",
    "The \"type\" category will be reduced to 3 classes by size:\n",
    "1. small: small sport utility vehicle, subcompact car, compact car, two seater, minicompact car, small station wagon\n",
    "2. medium: midsize car, standard sport utility vehicle, small pickup truck, midsize station wagon\n",
    "3. large: large car, minivan, standard pickup truck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b94039b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining region lists for make region categories\n",
    "asia = [\"kia\", \"hyundai\", \"genesis\", \"mazda\", \"honda\", \"acura\",\n",
    "    \"subaru\", \"mitsubishi\", \"toyota\", \"nissan\", \"infiniti\"]\n",
    "\n",
    "europe = [\"bmw\", \"jaguar\", \"mini\", \"audi\", \"land rover\", \"volvo\",\n",
    "    \"volkswagen\", \"aston martin\", \"porsche\", \"bentley\", \"mercedes-benz\"]\n",
    "\n",
    "america = [\"chevrolet\", \"jeep\", \"gmc\", \"ford\", \"cadillac\", \"buick\",\n",
    "    \"ram\", \"roush performance\", \"chrysler\"]\n",
    "\n",
    "def consolidate_region(make):\n",
    "    if make in asia:\n",
    "        return \"asia\"\n",
    "    if make in europe:\n",
    "        return \"europe\"\n",
    "    if make in america:\n",
    "        return \"america\"\n",
    "    return \"other\"   # just in case\n",
    "\n",
    "# consolidating make data to make_region\n",
    "data[\"make_region\"] = data[\"make\"].apply(consolidate_region)\n",
    "\n",
    "small = [\"small sport utility vehicle\",\"subcompact car\",\"compact car\",\n",
    "            \"two seater\",\"minicompact car\",\"small station wagon\"]\n",
    "medium = [\"midsize car\", \"standard sport utility vehicle\", \n",
    "            \"small pickup truck\", \"midsize station wagon\"]\n",
    "large = [\"large car\", \"minivan\", \"standard pickup truck\"]\n",
    "\n",
    "def consolidate_size(type):\n",
    "    if type in small:\n",
    "        return \"small\"\n",
    "    if type in medium:\n",
    "        return \"medium\"\n",
    "    if type in large:\n",
    "        return \"large\"\n",
    "    return \"other\"\n",
    "        \n",
    "# consolidating type data to size\n",
    "data[\"size\"] = data[\"type\"].apply(consolidate_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31321adc",
   "metadata": {},
   "source": [
    "**1.X: spomething**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90ed9030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature columns used for the model:\n",
      "['size', 'drive', 'fuel_type', 'make_region', 'transmission', 'cylinders', 'displacement']\n",
      "\n",
      "Number of samples after cleaning: 550\n"
     ]
    }
   ],
   "source": [
    "categorical_for_model_consolidated = ['size', 'drive', 'fuel_type', 'make_region', 'transmission']\n",
    "# Defining feature matrix X and target y\n",
    "feature_cols = categorical_for_model_consolidated + numerical\n",
    "X = data[feature_cols]\n",
    "y = data[output]\n",
    "\n",
    "print(\"\\nFeature columns used for the model:\")\n",
    "print(feature_cols)\n",
    "print(f\"\\nNumber of samples after cleaning: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f848c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9003eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d27c8a",
   "metadata": {},
   "source": [
    "## 2. Split off the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7202067c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train size: 440 samples\n",
      "Test size: 110 samples\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389100a",
   "metadata": {},
   "source": [
    "**2.X: One-hot encoding**\n",
    "Machine learning models like linear regression work with numbers, not strings.  \n",
    "We therefore need to convert categorical features into a numeric format.\n",
    "\n",
    "We use:\n",
    "\n",
    "- **`ColumnTransformer` with `OneHotEncoder`**  \n",
    "  - Each categorical column is expanded into one binary column per category.\n",
    "  - `handle_unknown='ignore'` ensures the model can handle categories that only appear in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "450bd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_for_model_consolidated)\n",
    "    ],\n",
    "    remainder='passthrough'  # keep numerical columns as-is\n",
    ")\n",
    "\n",
    "# 1. Fit preprocessor ONLY on training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# 2. Transform train and test sets\n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_test_enc  = preprocessor.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a147e0d",
   "metadata": {},
   "source": [
    "## 3. Choose families of models & hyperparameter variations to test:\n",
    "\n",
    "1. Linear Regression\n",
    "2. Forward Feature Reduction LR\n",
    "3. Neural Networks (MLP)  \n",
    "    3.1  \n",
    "    3.2  \n",
    "    3.3  \n",
    "    3.4  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410f406",
   "metadata": {},
   "source": [
    "## 4. Execute (train and evaluate) chosen models:\n",
    "[details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db06f0a",
   "metadata": {},
   "source": [
    "**4.1: Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b4de5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression R² Score on test set: 0.660\n"
     ]
    }
   ],
   "source": [
    "model_LR = LinearRegression()\n",
    "# Fit the linear regression model\n",
    "model_LR.fit(X_train_enc, y_train)\n",
    "\n",
    "# Predict and evaluate with R^2\n",
    "y_pred = model_LR.predict(X_test_enc)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\nLinear Regression R² Score on test set: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd62ea",
   "metadata": {},
   "source": [
    "**4.2: Linear Regression with forward feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b62924d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False  True  True  True False False  True  True False False\n",
      " False  True False  True  True]\n",
      "\n",
      "Forward Feature Selection Linear Regression R² Score on test set: 0.665\n"
     ]
    }
   ],
   "source": [
    "n_select = X_train_enc.shape[1] - 8 # number of features to keep\n",
    "\n",
    "estimator = model_LR\n",
    "#forward selection\n",
    "# we could use the cv cross validation parameter as a hyperparameter to optimize/play with\n",
    "sfs_forward = SequentialFeatureSelector(estimator, n_features_to_select=n_select, direction='forward',cv=5)\n",
    "\n",
    "sfs_forward.fit(X_train_enc,y_train)\n",
    "# Extract mask of selected features\n",
    "selected_mask = sfs_forward.get_support()\n",
    "print(selected_mask)\n",
    "\n",
    "# Reduce train/test matrices\n",
    "X_train_fs = X_train_enc[:, selected_mask]\n",
    "X_test_fs  = X_test_enc[:, selected_mask]\n",
    "\n",
    "# Fit a new model on selected features\n",
    "lr_fs = LinearRegression()\n",
    "lr_fs.fit(X_train_fs, y_train)\n",
    "\n",
    "# Predict + evaluate\n",
    "y_pred_forward = lr_fs.predict(X_test_fs)\n",
    "r2_forward = r2_score(y_test, y_pred_forward)\n",
    "print(f\"\\nForward Feature Selection Linear Regression R² Score on test set: {r2_forward:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f93ee",
   "metadata": {},
   "source": [
    "**4.X: Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a032b3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert to dense matrices for Keras\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train_enc_array \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m()\n\u001b[0;32m      3\u001b[0m X_test_enc_array  \u001b[38;5;241m=\u001b[39m X_test_enc\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal input feature dimension:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train_enc_array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "# Convert to dense matrices for Keras\n",
    "X_train_enc_array = X_train_enc.toarray()\n",
    "X_test_enc_array  = X_test_enc.toarray()\n",
    "\n",
    "print(\"Final input feature dimension:\", X_train_enc_array.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731f9de",
   "metadata": {},
   "source": [
    "**9. Neural Network Architectures**\n",
    "\n",
    "To explore non‑linear relationships, we implement four different neural network models:\n",
    "\n",
    "- **Model A — Simple**:  \n",
    "  - 1 hidden layer with 32 neurons (ReLU)  \n",
    "  - Good baseline, fast to train\n",
    "\n",
    "- **Model B — Deeper**:  \n",
    "  - Two hidden layers (64 → 32 units)  \n",
    "  - CAN model, more complex patterns\n",
    "\n",
    "- **Model C — Regularized with Dropout**:  \n",
    "  - Same idea as Model B but adds `Dropout(0.25)`  \n",
    "  - Helps reduce overfitting by randomly dropping neurons during training\n",
    "\n",
    "- **Model D — Wide with Batch Normalization**:  \n",
    "  - Two wide layers (128 → 64 units)  \n",
    "  - `BatchNormalization` after each hidd**en layer  \n",
    "  - Makes training more stable and can speed up convergence\n",
    "\n",
    "All models use:\n",
    "- **Optimizer**: Adam\n",
    "- **Loss**: Mean Squared Error (MSE)\n",
    "- **Metric**: Mean Absolute Error (MAE), easier to interpret as “average error in MPG”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== NN MODELS  ======\n",
    "\n",
    "def build_model_A(input_dim):\n",
    "    \"\"\"Model A - Simple 1-hidden-layer network.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_dim=input_dim),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_B(input_dim):\n",
    "    \"\"\"Model B - Deeper network with two hidden layers.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_C(input_dim):\n",
    "    \"\"\"Model C - Adds dropout for regularization.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_D(input_dim):\n",
    "    \"\"\"Model D - Wide network using batch normalization.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712afee",
   "metadata": {},
   "source": [
    "We train each neural network using:\n",
    "\n",
    "- Early stopping (`EarlyStopping`) with:  \n",
    "  - `patience=10`: if validation loss does not improve for 10 epochs, training stops  \n",
    "  - `restore_best_weights=True`: we keep the model from the best epoch\n",
    "\n",
    "For each model we:\n",
    "\n",
    "1. Train on the encoded training data with a validation split.\n",
    "2. Evaluate on the encoded test set to obtain MSE  and MAE.\n",
    "3. Plot the training and validation loss curves to see whether the model is overfitting or underfitting.\n",
    "\n",
    "This makes the comparison between different architectures transparent and well‑documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58776906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "models = {\n",
    "    \"Model A\": build_model_A(X_train_enc_array.shape[1]),\n",
    "    \"Model B\": build_model_B(X_train_enc_array.shape[1]),\n",
    "    \"Model C\": build_model_C(X_train_enc_array.shape[1]),\n",
    "    \"Model D\": build_model_D(X_train_enc_array.shape[1]),\n",
    "}\n",
    "\n",
    "history = {}\n",
    "results = {}\n",
    "\n",
    "for name, nn_model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    history[name] = nn_model.fit(\n",
    "        X_train_enc_array, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    test_mse, test_mae = nn_model.evaluate(X_test_enc_array, y_test, verbose=0)\n",
    "    results[name] = (test_mse, test_mae)\n",
    "    print(f\"{name} — Test MAE: {test_mae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643d115",
   "metadata": {},
   "source": [
    "**Visualizing Training Curves**  \n",
    "Finally, we plot the training loss and validation loss for each model across epochs.\n",
    "\n",
    "Things to look for:\n",
    "\n",
    "- If validation loss starts increasing while training loss keeps decreasing, then the model is overfitted.  \n",
    "- If both losses are high and do not improve, then the model might be underpowered or features might not be informative enough\n",
    "\n",
    "These plots help justify which neural network architectures are the most appropriate for this prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45728d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot training curves for each model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mhistory\u001b[49m:\n\u001b[0;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(history[name]\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training curves for each model\n",
    "for name in history:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history[name].history['loss'], label='Train Loss')\n",
    "    plt.plot(history[name].history['val_loss'], label='Val Loss')\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3d554",
   "metadata": {},
   "source": [
    "## 5: Choose best model and evaluate with test data:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
